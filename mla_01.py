# -*- coding: utf-8 -*-
"""MLA_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h_PDKvTmuDslVBzqAsBRwEyrd3QdwQ37

**MACHINE LEARNING ALGORITHM | PRACTICAL - 01**

**AIM:** Generate a proper 2-D data set of N points. Split the data set into Training Data set and
Test Data set. i) Perform linear regression analysis with Least Squares Method. ii) Plot
the graphs for Training MSE and Test MSE and comment on Curve Fitting and
Generalization Error. iii) Verify the Effect of Data Set Size and Bias-Variance Tradeoff.
iv) Apply Cross Validation and plot the graphs for errors. v) Apply Subset Selection
Method and plot the graphs for errors. vi) Describe your findings in each case
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import SelectKBest, f_regression

# Step 1: Generate a synthetic dataset
np.random.seed(42)
N = 100
X = np.linspace(0, 10, N)
y = 2*X + 3 + np.random.normal(0, 1, N)

# Step 2: Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 3: Perform linear regression analysis with Least Squares Method
model = LinearRegression()
X_train = X_train.reshape(-1, 1)
y_train = y_train.reshape(-1, 1)
model.fit(X_train, y_train)

# Step 4: Calculate MSE for training and test sets
X_test = X_test.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

# Step 5: Plot Training MSE and Test MSE
plt.plot(X_train, y_train, 'bo', label='Training Data')
plt.plot(X_test, y_test, 'ro', label='Test Data')
plt.plot(X_train, y_train_pred, 'g-', label='Training Predictions')
plt.plot(X_test, y_test_pred, 'm-', label='Test Predictions')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

# Step 6: Print curve fitting and generalization error
print(f"Training MSE: {train_mse}")
print(f"Test MSE: {test_mse}")

# Step 7: Apply Subset Selection Method and plot errors
k_values = range(1, N+1)
train_errors = []
test_errors = []

for k in k_values:
    top_k_features = np.argsort(f_regression(X_train, y_train)[0])[-k:]
    X_train_selected = X_train[:, top_k_features]
    X_test_selected = X_test[:, top_k_features]

    model.fit(X_train_selected, y_train)
    y_train_pred = model.predict(X_train_selected)
    y_test_pred = model.predict(X_test_selected)

    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)

    train_errors.append(train_mse)
    test_errors.append(test_mse)

plt.plot(k_values, train_errors, 'bo-', label='Training Error')
plt.plot(k_values, test_errors, 'ro-', label='Test Error')
plt.xlabel('Number of Features')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

# Step 9: Describe the findings
print("Findings:")
print("- The linear regression model fits the training data reasonably well, as seen from the plot.")
print("- The training MSE and test MSE can be used to assess the model's performance.")
print("- Subset selection helps identify the optimal number of features that minimize the prediction error.")